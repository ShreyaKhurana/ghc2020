{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT multilingual sentence embeddings\n",
    "\n",
    "For this we will be using the [bert-as-a-service](https://github.com/hanxiao/bert-as-service) module which starts a server to get BERT sentence embeddings of a sentence. To get this the library uses various pooling strategies over tokens. \n",
    "\n",
    "First install bert-serving-server and bert-serving-client by uncommenting the lines below. Note that this requires  1.10 <= PyTorch version <= 1.14 and the server needs to be Python3. The client can be either Py2 or Py3.\n",
    "\n",
    "Install [PyTorch](https://pytorch.org/get-started/locally/) for your required OS first.\n",
    "\n",
    "To know more about these sentence vectors are encoded, read [this](https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert-serving-server\n",
    "# !pip install bert-serving-client\n",
    "\n",
    "# Now download one of the bert models and run the bert server. Here I load the bert-base-cased\n",
    "# Note that this server needs TF 1.10<= version < 2.0, so I started the server on another virtualenv \n",
    "# !bert-serving-start -model_dir ~/Downloads/multi_cased_L-12_H-768_A-12 -num_worker=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/skhurana/pycon'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll encode sentences in different languages and try to see how their embeddings differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bc.encode([\"Two dogs are wrestling and hugging\", \n",
    "                       \"There is no dog wrestling and hugging\",\n",
    "                       \"दो कुत्ते कुश्ती और गले मिल रहे हैं\",\n",
    "                       \"कुश्ती और गले लगने वाले कुत्ते नहीं हैं\", \n",
    "                       \"Tengo muchas ganas de ir a la escuela\"\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85172975]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sentences[0][:].reshape(1,-1),sentences[2][:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8633849]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sentences[1][:].reshape(1,-1),sentences[3][:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8814724]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sentences[0][:].reshape(1,-1),sentences[4][:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Now we'll be using the [Huggingface library](https://github.com/huggingface/transformers) for the tokenizer and transformers. To see the installation part, see point 3 in [this](https://github.com/ShreyaKhurana/wwc/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import *\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ho', '##la', 'quiere', '##s', 'ir', 'al', 'centro', 'comercial', 'hoy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Hola quieres ir al centro comercial hoy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['च', '##ल', '##ो', 'आज', 'क', '##ही', '##ं', 'च', '##ल', '##ते', 'हैं']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"चलो आज कहीं चलते हैं\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bears',\n",
       " '.',\n",
       " 'Bee',\n",
       " '##ts',\n",
       " '.',\n",
       " 'Battle',\n",
       " '##star',\n",
       " 'Gall',\n",
       " '##act',\n",
       " '##ica',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Bears. Beets. Battlestar Gallactica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked LM\n",
    "\n",
    "Let's say you want to evaluate how the language model is performing over your multilingual corpus, which might have code-switched data like the example shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://mayhewsw.github.io/2019/01/16/can-bert-generate-text/\n",
    "def predict_word(text, target):\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(target)\n",
    "    tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Predict all tokens\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    predicted_index = torch.argmax(predictions[0][0, masked_index, :]).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "\n",
    "    print(\"Original:\", text)\n",
    "    print(\"Masked:\", \" \".join(tokenized_text))\n",
    "    print(\"Predicted token:\", predicted_token)\n",
    "    print(\"Other options:\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        predictions[0][0,masked_index,predicted_index] = -11100000\n",
    "        predicted_index = torch.argmax(predictions[0][0, masked_index]).item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "        print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'what', 'I', 'want', 'to', 'say', 'to', 'everyone', 'out', 'there', '-', 'vivir', 'y', 'dejar', 'vivir', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "This is what I want to say to everyone out there - vivir y dejar vivir!\n",
    "\"\"\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "This is what I want to say to everyone out there - vivir y dejar vivir!\n",
      "\n",
      "Masked: This is what I want to say to everyone out there - [MASK] y dejar vivir !\n",
      "Predicted token: ['¡']\n",
      "Other options:\n",
      "['...']\n",
      "['Viva']\n",
      "['Vive']\n",
      "['Hasta']\n",
      "['dejar']\n",
      "['Dar']\n",
      "['empezar']\n",
      "['Gracias']\n",
      "['volver']\n",
      "['Por']\n"
     ]
    }
   ],
   "source": [
    "predict_word(text, \"vivir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "This is what I want to say to everyone out there - vivir y dejar vivir!\n",
      "\n",
      "Masked: This is what I want to say to everyone out there - vivir y [MASK] vivir !\n",
      "Predicted token: ['no']\n",
      "Other options:\n",
      "['solo']\n",
      "['non']\n",
      "['bien']\n",
      "['nunca']\n",
      "['yo']\n",
      "['a']\n",
      "['sólo']\n",
      "['también']\n",
      "['hacer']\n",
      "[',']\n"
     ]
    }
   ],
   "source": [
    "# ¡Viva!\n",
    "predict_word(text, \"dejar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indian',\n",
       " 'mo',\n",
       " '##ms',\n",
       " 'at',\n",
       " '8',\n",
       " 'am',\n",
       " 'be',\n",
       " 'like',\n",
       " ':',\n",
       " '\"',\n",
       " 'ब',\n",
       " '##ेट',\n",
       " '##ा',\n",
       " ',',\n",
       " 'उ',\n",
       " '##ठ',\n",
       " '##ो',\n",
       " ',',\n",
       " '12',\n",
       " 'ब',\n",
       " '##ज',\n",
       " 'गए',\n",
       " 'हैं',\n",
       " '\"']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see a code-switched example.\n",
    "text = \"\"\"\n",
    "Indian moms at 8 am be like: \"बेटा, उठो, 12 बज गए हैं\"\n",
    "\"\"\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "Indian moms at 8 am be like: \"बेटा, उठो, 12 बज गए हैं\"\n",
      "\n",
      "Masked: Indian mo ##ms at 8 am be like : \" ब ##ेट ##ा , उ ##ठ ##ो , 12 ब ##ज [MASK] हैं \"\n",
      "Predicted token: ['##र']\n",
      "Other options:\n",
      "['##ते']\n",
      "['##ट']\n",
      "['##ू']\n",
      "['##ूत']\n",
      "['##ली']\n",
      "['##ती']\n",
      "['##ल']\n",
      "['##ु']\n",
      "['##ोर']\n",
      "['##त']\n"
     ]
    }
   ],
   "source": [
    "predict_word(text, \"गए\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the prediction is better for a monolingual simple example in Hindi\n",
    "text = \"\"\"\n",
    "भारतीय माँ सुबह 8 बजे - बेटा, उठो, 12 बज गए हैं\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['भारतीय',\n",
       " 'मा',\n",
       " '##ँ',\n",
       " 'स',\n",
       " '##ु',\n",
       " '##ब',\n",
       " '##ह',\n",
       " '8',\n",
       " 'ब',\n",
       " '##जे',\n",
       " '-',\n",
       " 'ब',\n",
       " '##ेट',\n",
       " '##ा',\n",
       " ',',\n",
       " 'उ',\n",
       " '##ठ',\n",
       " '##ो',\n",
       " ',',\n",
       " '12',\n",
       " 'ब',\n",
       " '##ज',\n",
       " 'गए',\n",
       " 'हैं']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "भारतीय माँ सुबह 8 बजे - बेटा, उठो, 12 बज गए हैं\n",
      "\n",
      "Masked: भारतीय [MASK] ##ँ स ##ु ##ब ##ह 8 ब ##जे - ब ##ेट ##ा , उ ##ठ ##ो , 12 ब ##ज गए हैं\n",
      "Predicted token: ['मा']\n",
      "Other options:\n",
      "['आ']\n",
      "['कहा']\n",
      "['जा']\n",
      "['अ']\n",
      "['खा']\n",
      "['हा']\n",
      "['औ']\n",
      "['का']\n",
      "['बी']\n",
      "['ऊ']\n"
     ]
    }
   ],
   "source": [
    "predict_word(text, \"मा\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['अ',\n",
       " '##गर',\n",
       " 'आ',\n",
       " '##प',\n",
       " 'हिंदी',\n",
       " 'में',\n",
       " 'जानकारी',\n",
       " 'प',\n",
       " '##ढ़',\n",
       " '##ना',\n",
       " 'प',\n",
       " '##स',\n",
       " '##ंद',\n",
       " 'करते',\n",
       " 'हैं',\n",
       " 'तो',\n",
       " 'आ',\n",
       " '##प',\n",
       " '##को',\n",
       " 'best',\n",
       " 'hindi',\n",
       " 'blog',\n",
       " '##s',\n",
       " 'या',\n",
       " '##नी',\n",
       " 'भारत',\n",
       " 'में',\n",
       " 'popular',\n",
       " 'Hindi',\n",
       " 'blog',\n",
       " '##gers',\n",
       " 'क',\n",
       " '##ौ',\n",
       " '##न',\n",
       " '-',\n",
       " 'क',\n",
       " '##ौ',\n",
       " '##न',\n",
       " 'से',\n",
       " 'हैं',\n",
       " 'इसकी',\n",
       " 'जानकारी',\n",
       " 'हो',\n",
       " '##नी',\n",
       " 'चाहिए',\n",
       " '।',\n",
       " 'बहुत',\n",
       " 'से',\n",
       " 'लोगों',\n",
       " 'को',\n",
       " 'इंग्लिश',\n",
       " 'में',\n",
       " 'जानकारी',\n",
       " 'होते',\n",
       " 'हुए',\n",
       " 'भी',\n",
       " 'व',\n",
       " '##ो',\n",
       " 'हिंदी',\n",
       " 'में',\n",
       " 'blog',\n",
       " 'प',\n",
       " '##ढ़',\n",
       " '##ना',\n",
       " 'प',\n",
       " '##स',\n",
       " '##ंद',\n",
       " 'करते',\n",
       " 'हैं',\n",
       " 'लेकिन',\n",
       " 'इसके',\n",
       " 'लिए',\n",
       " 'हम',\n",
       " '##ारे',\n",
       " 'पास',\n",
       " 'कुछ',\n",
       " 'popular',\n",
       " 'Hindi',\n",
       " 'blog',\n",
       " '##s',\n",
       " 'list',\n",
       " 'होने',\n",
       " 'चाहिए',\n",
       " '।']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try a bigger piece of text\n",
    "text = \"\"\"\n",
    "अगर आप हिंदी में जानकारी पढ़ना पसंद करते हैं तो आपको best hindi blogs यानी भारत में popular Hindi bloggers कौन-कौन से हैं इसकी जानकारी होनी चाहिए।\n",
    "\n",
    "बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी वो हिंदी में blog पढ़ना पसंद करते हैं लेकिन इसके लिए हमारे पास कुछ popular Hindi blogs list होने चाहिए।\n",
    "\"\"\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "अगर आप हिंदी में जानकारी पढ़ना पसंद करते हैं तो आपको best hindi blogs यानी भारत में popular Hindi bloggers कौन-कौन से हैं इसकी जानकारी होनी चाहिए।\n",
      "\n",
      "बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी वो हिंदी में blog पढ़ना पसंद करते हैं लेकिन इसके लिए हमारे पास कुछ popular Hindi blogs list होने चाहिए।\n",
      "\n",
      "Masked: अ ##गर आ ##प हिंदी में जानकारी प ##ढ़ ##ना प ##स ##ंद करते हैं तो आ ##प ##को best hindi [MASK] ##s या ##नी भारत में popular Hindi blog ##gers क ##ौ ##न - क ##ौ ##न से हैं इसकी जानकारी हो ##नी चाहिए । बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी व ##ो हिंदी में blog प ##ढ़ ##ना प ##स ##ंद करते हैं लेकिन इसके लिए हम ##ारे पास कुछ popular Hindi blog ##s list होने चाहिए ।\n",
      "Predicted token: ['blog']\n",
      "Other options:\n",
      "['Blog']\n",
      "['publisher']\n",
      "['forum']\n",
      "['tag']\n",
      "['poster']\n",
      "['photographer']\n",
      "['peer']\n",
      "['reporter']\n",
      "['guru']\n",
      "['pseudonym']\n"
     ]
    }
   ],
   "source": [
    "# It was able to recognize the mased word as its predicted first choice! But notice how useful the bidirectionality is here.\n",
    "predict_word(text, \"blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "अगर आप हिंदी में जानकारी पढ़ना पसंद करते हैं तो आपको best hindi blogs यानी भारत में popular Hindi bloggers कौन-कौन से हैं इसकी जानकारी होनी चाहिए।\n",
      "\n",
      "बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी वो हिंदी में blog पढ़ना पसंद करते हैं लेकिन इसके लिए हमारे पास कुछ popular Hindi blogs list होने चाहिए।\n",
      "\n",
      "Masked: अ ##गर आ ##प हिंदी में जानकारी प ##ढ़ ##ना प ##स ##ंद करते हैं तो आ ##प ##को best [MASK] blog ##s या ##नी भारत में popular Hindi blog ##gers क ##ौ ##न - क ##ौ ##न से हैं इसकी जानकारी हो ##नी चाहिए । बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी व ##ो हिंदी में blog प ##ढ़ ##ना प ##स ##ंद करते हैं लेकिन इसके लिए हम ##ारे पास कुछ popular Hindi blog ##s list होने चाहिए ।\n",
      "Predicted token: ['Hindi']\n",
      "Other options:\n",
      "['Indian']\n",
      "['English']\n",
      "['India']\n",
      "['Punjabi']\n",
      "['Bollywood']\n",
      "['of']\n",
      "['hindi']\n",
      "['known']\n",
      "['popular']\n",
      "['Pakistani']\n"
     ]
    }
   ],
   "source": [
    "# Let's try another word here\n",
    "predict_word(text, \"hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "अगर आप हिंदी में जानकारी पढ़ना पसंद करते हैं तो आपको best hindi blogs यानी भारत में popular Hindi bloggers कौन-कौन से हैं इसकी जानकारी होनी चाहिए।\n",
      "\n",
      "बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी वो हिंदी में blog पढ़ना पसंद करते हैं लेकिन इसके लिए हमारे पास कुछ popular Hindi blogs list होने चाहिए।\n",
      "\n",
      "Masked: अ ##गर आ ##प हिंदी में जानकारी प ##ढ़ ##ना प ##स ##ंद करते हैं तो आ ##प ##को best hindi blog ##s या ##नी भारत में popular Hindi blog ##gers क ##ौ ##न - क ##ौ ##न से हैं इसकी जानकारी हो ##नी चाहिए । बहुत से लोगों को [MASK] में जानकारी होते हुए भी व ##ो हिंदी में blog प ##ढ़ ##ना प ##स ##ंद करते हैं लेकिन इसके लिए हम ##ारे पास कुछ popular Hindi blog ##s list होने चाहिए ।\n",
      "Predicted token: ['हिंदी']\n",
      "Other options:\n",
      "['हिन्दी']\n",
      "['अंग्रेजी']\n",
      "['भारत']\n",
      "['अंग्रेज़ी']\n",
      "['मराठी']\n",
      "['उर्दू']\n",
      "['Hindi']\n",
      "['भाषा']\n",
      "['गुजराती']\n",
      "['इंग्रजी']\n"
     ]
    }
   ],
   "source": [
    "# Let's try another word here\n",
    "predict_word(text, \"इंग्लिश\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "अगर आप हिंदी में जानकारी पढ़ना पसंद करते हैं तो आपको best hindi blogs यानी भारत में popular Hindi bloggers कौन-कौन से हैं इसकी जानकारी होनी चाहिए।\n",
      "\n",
      "बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी वो हिंदी में blog पढ़ना पसंद करते हैं लेकिन इसके लिए हमारे पास कुछ popular Hindi blogs list होने चाहिए।\n",
      "\n",
      "Masked: अ ##गर आ ##प हिंदी में जानकारी प ##ढ़ ##ना प ##स ##ंद करते हैं तो आ ##प ##को best hindi blog ##s या ##नी भारत में popular Hindi blog ##gers क ##ौ ##न - क ##ौ ##न से हैं इसकी जानकारी हो ##नी चाहिए । बहुत से लोगों को इंग्लिश में जानकारी होते हुए भी व ##ो हिंदी में blog प ##ढ़ ##ना प ##स ##ंद करते हैं लेकिन इसके लिए हम ##ारे पास कुछ popular Hindi blog ##s [MASK] होने चाहिए ।\n",
      "Predicted token: ['भी']\n",
      "Other options:\n",
      "['नहीं']\n",
      "['उपलब्ध']\n",
      "['तैयार']\n",
      "['प्राप्त']\n",
      "['प्रकाशित']\n",
      "['ही']\n",
      "['प्रयोग']\n",
      "['न']\n",
      "['कम']\n",
      "['शामिल']\n"
     ]
    }
   ],
   "source": [
    "predict_word(text, \"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transliterated data\n",
    "\n",
    "\n",
    "Note that BERT has NOT been trained for this, it only has had access to multiple languages in their own script -> which is why we see pretty decent results above. But because it has been trained for languages in Roman scripts like English, Spanish, German etc. we might see some results, we would expect them to be gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aa',\n",
       " '##sma',\n",
       " 'Hai',\n",
       " 'Ne',\n",
       " '##ela',\n",
       " 'Ky',\n",
       " '##un',\n",
       " ',',\n",
       " 'Pa',\n",
       " '##ani',\n",
       " 'G',\n",
       " '##eel',\n",
       " '##a',\n",
       " 'G',\n",
       " '##eel',\n",
       " '##a',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Gol',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Hai',\n",
       " 'Za',\n",
       " '##meen',\n",
       " ',',\n",
       " 'Silk',\n",
       " 'Mein',\n",
       " 'Hai',\n",
       " 'Na',\n",
       " '##rmi',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Aa',\n",
       " '##g',\n",
       " 'Mein',\n",
       " 'Hai',\n",
       " 'Ga',\n",
       " '##rmi',\n",
       " 'Ky',\n",
       " '##un',\n",
       " ',',\n",
       " 'Do',\n",
       " 'Aur',\n",
       " 'Do',\n",
       " 'Pa',\n",
       " '##anc',\n",
       " '##h',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Na',\n",
       " '##hi',\n",
       " 'Pe',\n",
       " '##dd',\n",
       " 'Ho',\n",
       " 'Gay',\n",
       " '##e',\n",
       " 'Ka',\n",
       " '##m',\n",
       " 'Ky',\n",
       " '##un',\n",
       " ',',\n",
       " 'Teen',\n",
       " 'Hai',\n",
       " '##n',\n",
       " 'Ye',\n",
       " 'Mau',\n",
       " '##sam',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Chan',\n",
       " '##d',\n",
       " 'Do',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Na',\n",
       " '##hi',\n",
       " ',',\n",
       " 'Du',\n",
       " '##niya',\n",
       " 'Mein',\n",
       " 'Hai',\n",
       " 'Jung',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Be',\n",
       " '##hta',\n",
       " 'La',\n",
       " '##al',\n",
       " 'Rang',\n",
       " 'Ky',\n",
       " '##un',\n",
       " ',',\n",
       " 'Sar',\n",
       " '##had',\n",
       " '##ein',\n",
       " 'Hai',\n",
       " '##n',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Har',\n",
       " 'Ka',\n",
       " '##hin',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'Ye',\n",
       " '##h',\n",
       " 'Tu',\n",
       " '##mne',\n",
       " 'Ky',\n",
       " '##a',\n",
       " 'Ka',\n",
       " '##bh',\n",
       " '##i',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'Ki',\n",
       " 'Hai',\n",
       " '##n',\n",
       " 'Ye',\n",
       " '##h',\n",
       " 'Ky',\n",
       " '##a',\n",
       " 'Sa',\n",
       " '##bh',\n",
       " '##i',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Na',\n",
       " '##hi',\n",
       " 'To',\n",
       " 'Soc',\n",
       " '##ho',\n",
       " 'Ab',\n",
       " '##hi',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'Be',\n",
       " '##hti',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Hai',\n",
       " 'Har',\n",
       " 'Nad',\n",
       " '##i',\n",
       " ',',\n",
       " 'Hot',\n",
       " '##i',\n",
       " 'Ky',\n",
       " '##a',\n",
       " 'Hai',\n",
       " 'Ros',\n",
       " '##hn',\n",
       " '##i',\n",
       " 'Bar',\n",
       " '##f',\n",
       " 'G',\n",
       " '##irt',\n",
       " '##i',\n",
       " 'Hai',\n",
       " 'Ky',\n",
       " '##un',\n",
       " ',',\n",
       " 'Dos',\n",
       " '##t',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Hai',\n",
       " '##n',\n",
       " 'Root',\n",
       " '##hte',\n",
       " 'Ta',\n",
       " '##are',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Hai',\n",
       " '##n',\n",
       " 'Tutte',\n",
       " '##y',\n",
       " ',',\n",
       " 'Ba',\n",
       " '##ad',\n",
       " '##lon',\n",
       " 'Mein',\n",
       " 'Bij',\n",
       " '##li',\n",
       " 'Hai',\n",
       " 'Ky',\n",
       " '##un',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'Ye',\n",
       " '##h',\n",
       " 'Tu',\n",
       " '##mne',\n",
       " 'Ky',\n",
       " '##a',\n",
       " 'Ka',\n",
       " '##bh',\n",
       " '##i',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'Ki',\n",
       " 'Hai',\n",
       " '##n',\n",
       " 'Ye',\n",
       " '##h',\n",
       " 'Ky',\n",
       " '##a',\n",
       " 'Sa',\n",
       " '##bh',\n",
       " '##i',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Na',\n",
       " '##hi',\n",
       " 'To',\n",
       " 'Soc',\n",
       " '##ho',\n",
       " 'Ab',\n",
       " '##hi',\n",
       " 'San',\n",
       " '##nata',\n",
       " 'Sun',\n",
       " '##ai',\n",
       " 'Na',\n",
       " '##hin',\n",
       " 'Det',\n",
       " '##a',\n",
       " ',',\n",
       " 'Aur',\n",
       " 'Ha',\n",
       " '##way',\n",
       " '##en',\n",
       " 'Di',\n",
       " '##kha',\n",
       " '##yi',\n",
       " 'Na',\n",
       " '##hin',\n",
       " 'Det',\n",
       " '##i',\n",
       " 'Soc',\n",
       " '##ha',\n",
       " 'Hai',\n",
       " 'Ky',\n",
       " '##a',\n",
       " 'Ka',\n",
       " '##bh',\n",
       " '##i',\n",
       " ',',\n",
       " 'Hot',\n",
       " '##a',\n",
       " 'Hai',\n",
       " 'Ye',\n",
       " '##h',\n",
       " 'Ky',\n",
       " '##un']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A common place where you will find transliterated data is lyrics and thanks to Bollywood, Hindi has plenty!!\n",
    "text = \"\"\"Aasma Hai Neela Kyun, Paani Geela Geela Kyun\n",
    "Gol Kyun Hai Zameen, Silk Mein Hai Narmi Kyun\n",
    "Aag Mein Hai Garmi Kyun, Do Aur Do Paanch Kyun Nahi\n",
    "Pedd Ho Gaye Kam Kyun, Teen Hain Ye Mausam Kyun\n",
    "Chand Do Kyun Nahi, Duniya Mein Hai Jung Kyun\n",
    "Behta Laal Rang Kyun, Sarhadein Hain Kyun Har Kahin\n",
    "Socha Hai... Yeh Tumne Kya Kabhi\n",
    "Socha Hai... Ki Hain Yeh Kya Sabhi Socha Hai\n",
    "Socha Nahi To Socho Abhi.....\n",
    "Behti Kyun Hai Har Nadi, Hoti Kya Hai Roshni\n",
    "Barf Girti Hai Kyun, Dost Kyun Hain Roothte\n",
    "Taare Kyun Hain Tuttey, Baadlon Mein Bijli Hai Kyun\n",
    "Socha Hai... Yeh Tumne Kya Kabhi\n",
    "Socha Hai... Ki Hain Yeh Kya Sabhi Socha Hai\n",
    "Socha Nahi To Socho Abhi\n",
    "Sannata Sunai Nahin Deta, Aur Hawayen Dikhayi Nahin Deti\n",
    "Socha Hai Kya Kabhi, Hota Hai Yeh Kyun\n",
    "\"\"\"\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Aasma Hai Neela Kyun, Paani Geela Geela Kyun\n",
      "Gol Kyun Hai Zameen, Silk Mein Hai Narmi Kyun\n",
      "Aag Mein Hai Garmi Kyun, Do Aur Do Paanch Kyun Nahi\n",
      "Pedd Ho Gaye Kam Kyun, Teen Hain Ye Mausam Kyun\n",
      "Chand Do Kyun Nahi, Duniya Mein Hai Jung Kyun\n",
      "Behta Laal Rang Kyun, Sarhadein Hain Kyun Har Kahin\n",
      "Socha Hai... Yeh Tumne Kya Kabhi\n",
      "Socha Hai... Ki Hain Yeh Kya Sabhi Socha Hai\n",
      "Socha Nahi To Socho Abhi.....\n",
      "Behti Kyun Hai Har Nadi, Hoti Kya Hai Roshni\n",
      "Barf Girti Hai Kyun, Dost Kyun Hain Roothte\n",
      "Taare Kyun Hain Tuttey, Baadlon Mein Bijli Hai Kyun\n",
      "Socha Hai... Yeh Tumne Kya Kabhi\n",
      "Socha Hai... Ki Hain Yeh Kya Sabhi Socha Hai\n",
      "Socha Nahi To Socho Abhi\n",
      "Sannata Sunai Nahin Deta, Aur Hawayen Dikhayi Nahin Deti\n",
      "Socha Hai Kya Kabhi, Hota Hai Yeh Kyun\n",
      "\n",
      "Masked: Aa ##sma Hai Ne ##ela Ky ##un , Pa ##ani G ##eel ##a G ##eel ##a Ky ##un [MASK] Ky ##un Hai Za ##meen , Silk Mein Hai Na ##rmi Ky ##un Aa ##g Mein Hai Ga ##rmi Ky ##un , Do Aur Do Pa ##anc ##h Ky ##un Na ##hi Pe ##dd Ho Gay ##e Ka ##m Ky ##un , Teen Hai ##n Ye Mau ##sam Ky ##un Chan ##d Do Ky ##un Na ##hi , Du ##niya Mein Hai Jung Ky ##un Be ##hta La ##al Rang Ky ##un , Sar ##had ##ein Hai ##n Ky ##un Har Ka ##hin Soc ##ha Hai . . . Ye ##h Tu ##mne Ky ##a Ka ##bh ##i Soc ##ha Hai . . . Ki Hai ##n Ye ##h Ky ##a Sa ##bh ##i Soc ##ha Hai Soc ##ha Na ##hi To Soc ##ho Ab ##hi . . . . . Be ##hti Ky ##un Hai Har Nad ##i , Hot ##i Ky ##a Hai Ros ##hn ##i Bar ##f G ##irt ##i Hai Ky ##un , Dos ##t Ky ##un Hai ##n Root ##hte Ta ##are Ky ##un Hai ##n Tutte ##y , Ba ##ad ##lon Mein Bij ##li Hai Ky ##un Soc ##ha Hai . . . Ye ##h Tu ##mne Ky ##a Ka ##bh ##i Soc ##ha Hai . . . Ki Hai ##n Ye ##h Ky ##a Sa ##bh ##i Soc ##ha Hai Soc ##ha Na ##hi To Soc ##ho Ab ##hi San ##nata Sun ##ai Na ##hin Det ##a , Aur Ha ##way ##en Di ##kha ##yi Na ##hin Det ##i Soc ##ha Hai Ky ##a Ka ##bh ##i , Hot ##a Hai Ye ##h Ky ##un\n",
      "Predicted token: [',']\n",
      "Other options:\n",
      "['.']\n",
      "['Hai']\n",
      "['...']\n",
      "['-']\n",
      "['Ki']\n",
      "['Ka']\n",
      "['Dil']\n",
      "['Kara']\n",
      "['(']\n",
      "[')']\n"
     ]
    }
   ],
   "source": [
    "predict_word(text, \"Gol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Aasma Hai Neela Kyun, Paani Geela Geela Kyun\n",
      "Gol Kyun Hai Zameen, Silk Mein Hai Narmi Kyun\n",
      "Aag Mein Hai Garmi Kyun, Do Aur Do Paanch Kyun Nahi\n",
      "Pedd Ho Gaye Kam Kyun, Teen Hain Ye Mausam Kyun\n",
      "Chand Do Kyun Nahi, Duniya Mein Hai Jung Kyun\n",
      "Behta Laal Rang Kyun, Sarhadein Hain Kyun Har Kahin\n",
      "Socha Hai... Yeh Tumne Kya Kabhi\n",
      "Socha Hai... Ki Hain Yeh Kya Sabhi Socha Hai\n",
      "Socha Nahi To Socho Abhi.....\n",
      "Behti Kyun Hai Har Nadi, Hoti Kya Hai Roshni\n",
      "Barf Girti Hai Kyun, Dost Kyun Hain Roothte\n",
      "Taare Kyun Hain Tuttey, Baadlon Mein Bijli Hai Kyun\n",
      "Socha Hai... Yeh Tumne Kya Kabhi\n",
      "Socha Hai... Ki Hain Yeh Kya Sabhi Socha Hai\n",
      "Socha Nahi To Socho Abhi\n",
      "Sannata Sunai Nahin Deta, Aur Hawayen Dikhayi Nahin Deti\n",
      "Socha Hai Kya Kabhi, Hota Hai Yeh Kyun\n",
      "\n",
      "Masked: Aa ##sma Hai Ne ##ela Ky ##un , Pa ##ani G ##eel ##a G ##eel ##a Ky ##un Gol Ky ##un Hai Za ##meen , Silk [MASK] Hai Na ##rmi Ky ##un Aa ##g Mein Hai Ga ##rmi Ky ##un , Do Aur Do Pa ##anc ##h Ky ##un Na ##hi Pe ##dd Ho Gay ##e Ka ##m Ky ##un , Teen Hai ##n Ye Mau ##sam Ky ##un Chan ##d Do Ky ##un Na ##hi , Du ##niya Mein Hai Jung Ky ##un Be ##hta La ##al Rang Ky ##un , Sar ##had ##ein Hai ##n Ky ##un Har Ka ##hin Soc ##ha Hai . . . Ye ##h Tu ##mne Ky ##a Ka ##bh ##i Soc ##ha Hai . . . Ki Hai ##n Ye ##h Ky ##a Sa ##bh ##i Soc ##ha Hai Soc ##ha Na ##hi To Soc ##ho Ab ##hi . . . . . Be ##hti Ky ##un Hai Har Nad ##i , Hot ##i Ky ##a Hai Ros ##hn ##i Bar ##f G ##irt ##i Hai Ky ##un , Dos ##t Ky ##un Hai ##n Root ##hte Ta ##are Ky ##un Hai ##n Tutte ##y , Ba ##ad ##lon Mein Bij ##li Hai Ky ##un Soc ##ha Hai . . . Ye ##h Tu ##mne Ky ##a Ka ##bh ##i Soc ##ha Hai . . . Ki Hai ##n Ye ##h Ky ##a Sa ##bh ##i Soc ##ha Hai Soc ##ha Na ##hi To Soc ##ho Ab ##hi San ##nata Sun ##ai Na ##hin Det ##a , Aur Ha ##way ##en Di ##kha ##yi Na ##hin Det ##i Soc ##ha Hai Ky ##a Ka ##bh ##i , Hot ##a Hai Ye ##h Ky ##un\n",
      "Predicted token: ['Mein']\n",
      "Other options:\n",
      "['##e']\n",
      "['##en']\n",
      "['##y']\n",
      "['##i']\n",
      "['##ei']\n",
      "['Hai']\n",
      "['##in']\n",
      "['##a']\n",
      "['##em']\n",
      "['##er']\n"
     ]
    }
   ],
   "source": [
    "predict_word(text, \"Mein\")\n",
    "# Notice WHY it was able to correctly predict it -> it has a couple of other similar contexts to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
